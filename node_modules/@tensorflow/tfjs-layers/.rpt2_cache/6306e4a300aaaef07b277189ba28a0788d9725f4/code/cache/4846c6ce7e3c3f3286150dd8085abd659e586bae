{"code":"import * as tslib_1 from \"tslib\";\r\nimport { elu, leakyRelu, serialization } from '@tensorflow/tfjs-core';\r\nimport { Softmax as softmaxActivation } from '../activations';\r\nimport { cast } from '../backend/tfjs_backend';\r\nimport { getScalar } from '../backend/tfjs_backend';\r\nimport { Layer } from '../engine/topology';\r\nimport { NotImplementedError } from '../errors';\r\nimport * as generic_utils from '../utils/generic_utils';\r\nvar LeakyReLU = (function (_super) {\r\n    tslib_1.__extends(LeakyReLU, _super);\r\n    function LeakyReLU(config) {\r\n        var _this = _super.call(this, config == null ? {} : config) || this;\r\n        _this.DEFAULT_ALPHA = 0.3;\r\n        if (config == null) {\r\n            config = {};\r\n        }\r\n        _this.alpha = config.alpha == null ? _this.DEFAULT_ALPHA : config.alpha;\r\n        return _this;\r\n    }\r\n    LeakyReLU.prototype.call = function (inputs, kwargs) {\r\n        var x = generic_utils.getExactlyOneTensor(inputs);\r\n        return leakyRelu(x, this.alpha);\r\n    };\r\n    LeakyReLU.prototype.computeOutputShape = function (inputShape) {\r\n        return inputShape;\r\n    };\r\n    LeakyReLU.prototype.getConfig = function () {\r\n        var config = { alpha: this.alpha };\r\n        var baseConfig = _super.prototype.getConfig.call(this);\r\n        Object.assign(config, baseConfig);\r\n        return config;\r\n    };\r\n    LeakyReLU.className = 'LeakyReLU';\r\n    return LeakyReLU;\r\n}(Layer));\r\nexport { LeakyReLU };\r\nserialization.SerializationMap.register(LeakyReLU);\r\nvar ELU = (function (_super) {\r\n    tslib_1.__extends(ELU, _super);\r\n    function ELU(config) {\r\n        var _this = _super.call(this, config == null ? {} : config) || this;\r\n        _this.DEFAULT_ALPHA = 1.0;\r\n        if (config == null) {\r\n            config = {};\r\n        }\r\n        if (config.alpha != null && config.alpha !== _this.DEFAULT_ALPHA) {\r\n            throw new NotImplementedError(\"Non-default alpha value (\" + config.alpha + \") is not supported by the \" +\r\n                \"ELU layer yet.\");\r\n        }\r\n        _this.alpha = config.alpha == null ? _this.DEFAULT_ALPHA : config.alpha;\r\n        return _this;\r\n    }\r\n    ELU.prototype.call = function (inputs, kwargs) {\r\n        var x = generic_utils.getExactlyOneTensor(inputs);\r\n        return elu(x);\r\n    };\r\n    ELU.prototype.computeOutputShape = function (inputShape) {\r\n        return inputShape;\r\n    };\r\n    ELU.prototype.getConfig = function () {\r\n        var config = { alpha: this.alpha };\r\n        var baseConfig = _super.prototype.getConfig.call(this);\r\n        Object.assign(config, baseConfig);\r\n        return config;\r\n    };\r\n    ELU.className = 'ELU';\r\n    return ELU;\r\n}(Layer));\r\nexport { ELU };\r\nserialization.SerializationMap.register(ELU);\r\nvar ThresholdedReLU = (function (_super) {\r\n    tslib_1.__extends(ThresholdedReLU, _super);\r\n    function ThresholdedReLU(config) {\r\n        var _this = _super.call(this, config == null ? {} : config) || this;\r\n        _this.DEFAULT_THETA = 1.0;\r\n        if (config == null) {\r\n            config = {};\r\n        }\r\n        _this.theta = config.theta == null ? _this.DEFAULT_THETA : config.theta;\r\n        _this.thetaTensor = getScalar(_this.theta);\r\n        return _this;\r\n    }\r\n    ThresholdedReLU.prototype.call = function (inputs, kwargs) {\r\n        var x = generic_utils.getExactlyOneTensor(inputs);\r\n        return x.mul(cast(x.greater(this.thetaTensor), 'float32'));\r\n    };\r\n    ThresholdedReLU.prototype.computeOutputShape = function (inputShape) {\r\n        return inputShape;\r\n    };\r\n    ThresholdedReLU.prototype.getConfig = function () {\r\n        var config = { theta: this.theta };\r\n        var baseConfig = _super.prototype.getConfig.call(this);\r\n        Object.assign(config, baseConfig);\r\n        return config;\r\n    };\r\n    ThresholdedReLU.className = 'ThresholdedReLU';\r\n    return ThresholdedReLU;\r\n}(Layer));\r\nexport { ThresholdedReLU };\r\nserialization.SerializationMap.register(ThresholdedReLU);\r\nvar Softmax = (function (_super) {\r\n    tslib_1.__extends(Softmax, _super);\r\n    function Softmax(config) {\r\n        var _this = _super.call(this, config == null ? {} : config) || this;\r\n        _this.DEFAULT_AXIS = 1.0;\r\n        if (config == null) {\r\n            config = {};\r\n        }\r\n        _this.softmax = new softmaxActivation().apply;\r\n        _this.axis = config.axis == null ? _this.DEFAULT_AXIS : config.axis;\r\n        return _this;\r\n    }\r\n    Softmax.prototype.call = function (inputs, kwargs) {\r\n        var x = generic_utils.getExactlyOneTensor(inputs);\r\n        return this.softmax(x, this.axis);\r\n    };\r\n    Softmax.prototype.computeOutputShape = function (inputShape) {\r\n        return inputShape;\r\n    };\r\n    Softmax.prototype.getConfig = function () {\r\n        var config = { axis: this.axis };\r\n        var baseConfig = _super.prototype.getConfig.call(this);\r\n        Object.assign(config, baseConfig);\r\n        return config;\r\n    };\r\n    Softmax.className = 'Softmax';\r\n    return Softmax;\r\n}(Layer));\r\nexport { Softmax };\r\nserialization.SerializationMap.register(Softmax);\r\n//# sourceMappingURL=advanced_activations.js.map","map":"{\"version\":3,\"file\":\"advanced_activations.js\",\"sourceRoot\":\"\",\"sources\":[\"../src/layers/advanced_activations.ts\"],\"names\":[],\"mappings\":\";AAcA,OAAO,EAAC,GAAG,EAAE,SAAS,EAAE,aAAa,EAAS,MAAM,uBAAuB,CAAC;AAE5E,OAAO,EAAC,OAAO,IAAI,iBAAiB,EAAC,MAAM,gBAAgB,CAAC;AAC5D,OAAO,EAAC,IAAI,EAAC,MAAM,yBAAyB,CAAC;AAC7C,OAAO,EAAC,SAAS,EAAC,MAAM,yBAAyB,CAAC;AAClD,OAAO,EAAC,KAAK,EAAc,MAAM,oBAAoB,CAAC;AACtD,OAAO,EAAC,mBAAmB,EAAC,MAAM,WAAW,CAAC;AAE9C,OAAO,KAAK,aAAa,MAAM,wBAAwB,CAAC;AAuBxD;IAA+B,qCAAK;IAMlC,mBAAY,MAA6B;QAAzC,YACE,kBAAM,MAAM,IAAI,IAAI,CAAC,CAAC,CAAC,EAAE,CAAC,CAAC,CAAC,MAAM,CAAC,SAMpC;QATQ,mBAAa,GAAG,GAAG,CAAC;QAI3B,IAAI,MAAM,IAAI,IAAI,EAAE;YAClB,MAAM,GAAG,EAAE,CAAC;SACb;QAED,KAAI,CAAC,KAAK,GAAG,MAAM,CAAC,KAAK,IAAI,IAAI,CAAC,CAAC,CAAC,KAAI,CAAC,aAAa,CAAC,CAAC,CAAC,MAAM,CAAC,KAAK,CAAC;;IACxE,CAAC;IAED,wBAAI,GAAJ,UAAK,MAAuB,EAAE,MAAc;QAC1C,IAAM,CAAC,GAAG,aAAa,CAAC,mBAAmB,CAAC,MAAM,CAAC,CAAC;QACpD,OAAO,SAAS,CAAC,CAAC,EAAE,IAAI,CAAC,KAAK,CAAC,CAAC;IAClC,CAAC;IAED,sCAAkB,GAAlB,UAAmB,UAAyB;QAC1C,OAAO,UAAU,CAAC;IACpB,CAAC;IAED,6BAAS,GAAT;QACE,IAAM,MAAM,GAA6B,EAAC,KAAK,EAAE,IAAI,CAAC,KAAK,EAAC,CAAC;QAC7D,IAAM,UAAU,GAAG,iBAAM,SAAS,WAAE,CAAC;QACrC,MAAM,CAAC,MAAM,CAAC,MAAM,EAAE,UAAU,CAAC,CAAC;QAClC,OAAO,MAAM,CAAC;IAChB,CAAC;IA5BM,mBAAS,GAAG,WAAW,CAAC;IA6BjC,gBAAC;CAAA,AA9BD,CAA+B,KAAK,GA8BnC;SA9BY,SAAS;AA+BtB,aAAa,CAAC,gBAAgB,CAAC,QAAQ,CAAC,SAAS,CAAC,CAAC;AA6BnD;IAAyB,+BAAK;IAM5B,aAAY,MAAuB;QAAnC,YACE,kBAAM,MAAM,IAAI,IAAI,CAAC,CAAC,CAAC,EAAE,CAAC,CAAC,CAAC,MAAM,CAAC,SAYpC;QAfQ,mBAAa,GAAG,GAAG,CAAC;QAI3B,IAAI,MAAM,IAAI,IAAI,EAAE;YAClB,MAAM,GAAG,EAAE,CAAC;SACb;QAED,IAAI,MAAM,CAAC,KAAK,IAAI,IAAI,IAAI,MAAM,CAAC,KAAK,KAAK,KAAI,CAAC,aAAa,EAAE;YAC/D,MAAM,IAAI,mBAAmB,CACzB,8BAA4B,MAAM,CAAC,KAAK,+BAA4B;gBACpE,gBAAgB,CAAC,CAAC;SACvB;QAED,KAAI,CAAC,KAAK,GAAG,MAAM,CAAC,KAAK,IAAI,IAAI,CAAC,CAAC,CAAC,KAAI,CAAC,aAAa,CAAC,CAAC,CAAC,MAAM,CAAC,KAAK,CAAC;;IACxE,CAAC;IAED,kBAAI,GAAJ,UAAK,MAAuB,EAAE,MAAc;QAC1C,IAAM,CAAC,GAAG,aAAa,CAAC,mBAAmB,CAAC,MAAM,CAAC,CAAC;QACpD,OAAO,GAAG,CAAC,CAAC,CAAC,CAAC;IAChB,CAAC;IAED,gCAAkB,GAAlB,UAAmB,UAAyB;QAC1C,OAAO,UAAU,CAAC;IACpB,CAAC;IAED,uBAAS,GAAT;QACE,IAAM,MAAM,GAA6B,EAAC,KAAK,EAAE,IAAI,CAAC,KAAK,EAAC,CAAC;QAC7D,IAAM,UAAU,GAAG,iBAAM,SAAS,WAAE,CAAC;QACrC,MAAM,CAAC,MAAM,CAAC,MAAM,EAAE,UAAU,CAAC,CAAC;QAClC,OAAO,MAAM,CAAC;IAChB,CAAC;IAlCM,aAAS,GAAG,KAAK,CAAC;IAmC3B,UAAC;CAAA,AApCD,CAAyB,KAAK,GAoC7B;SApCY,GAAG;AAqChB,aAAa,CAAC,gBAAgB,CAAC,QAAQ,CAAC,GAAG,CAAC,CAAC;AA2B7C;IAAqC,2CAAK;IAOxC,yBAAY,MAAmC;QAA/C,YACE,kBAAM,MAAM,IAAI,IAAI,CAAC,CAAC,CAAC,EAAE,CAAC,CAAC,CAAC,MAAM,CAAC,SAOpC;QAVQ,mBAAa,GAAG,GAAG,CAAC;QAI3B,IAAI,MAAM,IAAI,IAAI,EAAE;YAClB,MAAM,GAAG,EAAE,CAAC;SACb;QAED,KAAI,CAAC,KAAK,GAAG,MAAM,CAAC,KAAK,IAAI,IAAI,CAAC,CAAC,CAAC,KAAI,CAAC,aAAa,CAAC,CAAC,CAAC,MAAM,CAAC,KAAK,CAAC;QACtE,KAAI,CAAC,WAAW,GAAG,SAAS,CAAC,KAAI,CAAC,KAAK,CAAC,CAAC;;IAC3C,CAAC;IAED,8BAAI,GAAJ,UAAK,MAAuB,EAAE,MAAc;QAC1C,IAAM,CAAC,GAAG,aAAa,CAAC,mBAAmB,CAAC,MAAM,CAAC,CAAC;QACpD,OAAO,CAAC,CAAC,GAAG,CAAC,IAAI,CAAC,CAAC,CAAC,OAAO,CAAC,IAAI,CAAC,WAAW,CAAC,EAAE,SAAS,CAAC,CAAC,CAAC;IAC7D,CAAC;IAED,4CAAkB,GAAlB,UAAmB,UAAyB;QAC1C,OAAO,UAAU,CAAC;IACpB,CAAC;IAED,mCAAS,GAAT;QACE,IAAM,MAAM,GAA6B,EAAC,KAAK,EAAE,IAAI,CAAC,KAAK,EAAC,CAAC;QAC7D,IAAM,UAAU,GAAG,iBAAM,SAAS,WAAE,CAAC;QACrC,MAAM,CAAC,MAAM,CAAC,MAAM,EAAE,UAAU,CAAC,CAAC;QAClC,OAAO,MAAM,CAAC;IAChB,CAAC;IA9BM,yBAAS,GAAG,iBAAiB,CAAC;IA+BvC,sBAAC;CAAA,AAhCD,CAAqC,KAAK,GAgCzC;SAhCY,eAAe;AAiC5B,aAAa,CAAC,gBAAgB,CAAC,QAAQ,CAAC,eAAe,CAAC,CAAC;AAoBzD;IAA6B,mCAAK;IAMhC,iBAAY,MAA2B;QAAvC,YACE,kBAAM,MAAM,IAAI,IAAI,CAAC,CAAC,CAAC,EAAE,CAAC,CAAC,CAAC,MAAM,CAAC,SAMpC;QATQ,kBAAY,GAAG,GAAG,CAAC;QAI1B,IAAI,MAAM,IAAI,IAAI,EAAE;YAClB,MAAM,GAAG,EAAE,CAAC;SACb;QACD,KAAI,CAAC,OAAO,GAAG,IAAI,iBAAiB,EAAE,CAAC,KAAK,CAAC;QAC7C,KAAI,CAAC,IAAI,GAAG,MAAM,CAAC,IAAI,IAAI,IAAI,CAAC,CAAC,CAAC,KAAI,CAAC,YAAY,CAAC,CAAC,CAAC,MAAM,CAAC,IAAI,CAAC;;IACpE,CAAC;IAED,sBAAI,GAAJ,UAAK,MAAuB,EAAE,MAAc;QAC1C,IAAM,CAAC,GAAG,aAAa,CAAC,mBAAmB,CAAC,MAAM,CAAC,CAAC;QACpD,OAAO,IAAI,CAAC,OAAO,CAAC,CAAC,EAAE,IAAI,CAAC,IAAI,CAAC,CAAC;IACpC,CAAC;IAED,oCAAkB,GAAlB,UAAmB,UAAyB;QAC1C,OAAO,UAAU,CAAC;IACpB,CAAC;IAED,2BAAS,GAAT;QACE,IAAM,MAAM,GAA6B,EAAC,IAAI,EAAE,IAAI,CAAC,IAAI,EAAC,CAAC;QAC3D,IAAM,UAAU,GAAG,iBAAM,SAAS,WAAE,CAAC;QACrC,MAAM,CAAC,MAAM,CAAC,MAAM,EAAE,UAAU,CAAC,CAAC;QAClC,OAAO,MAAM,CAAC;IAChB,CAAC;IA5BM,iBAAS,GAAG,SAAS,CAAC;IA6B/B,cAAC;CAAA,AA9BD,CAA6B,KAAK,GA8BjC;SA9BY,OAAO;AA+BpB,aAAa,CAAC,gBAAgB,CAAC,QAAQ,CAAC,OAAO,CAAC,CAAC\"}","dts":{"name":"/usr/local/google/home/smilkov/code/tfjs-layers/layers/advanced_activations.d.ts","text":"import { serialization, Tensor } from '@tensorflow/tfjs-core';\r\nimport { Layer, LayerConfig } from '../engine/topology';\r\nimport { Kwargs, Shape } from '../types';\r\nexport interface LeakyReLULayerConfig extends LayerConfig {\r\n    alpha?: number;\r\n}\r\nexport declare class LeakyReLU extends Layer {\r\n    static className: string;\r\n    readonly alpha: number;\r\n    readonly DEFAULT_ALPHA: number;\r\n    constructor(config?: LeakyReLULayerConfig);\r\n    call(inputs: Tensor | Tensor[], kwargs: Kwargs): Tensor | Tensor[];\r\n    computeOutputShape(inputShape: Shape | Shape[]): Shape | Shape[];\r\n    getConfig(): serialization.ConfigDict;\r\n}\r\nexport interface ELULayerConfig extends LayerConfig {\r\n    alpha?: number;\r\n}\r\nexport declare class ELU extends Layer {\r\n    static className: string;\r\n    readonly alpha: number;\r\n    readonly DEFAULT_ALPHA: number;\r\n    constructor(config?: ELULayerConfig);\r\n    call(inputs: Tensor | Tensor[], kwargs: Kwargs): Tensor | Tensor[];\r\n    computeOutputShape(inputShape: Shape | Shape[]): Shape | Shape[];\r\n    getConfig(): serialization.ConfigDict;\r\n}\r\nexport interface ThresholdedReLULayerConfig extends LayerConfig {\r\n    theta?: number;\r\n}\r\nexport declare class ThresholdedReLU extends Layer {\r\n    static className: string;\r\n    readonly theta: number;\r\n    private readonly thetaTensor;\r\n    readonly DEFAULT_THETA: number;\r\n    constructor(config?: ThresholdedReLULayerConfig);\r\n    call(inputs: Tensor | Tensor[], kwargs: Kwargs): Tensor | Tensor[];\r\n    computeOutputShape(inputShape: Shape | Shape[]): Shape | Shape[];\r\n    getConfig(): serialization.ConfigDict;\r\n}\r\nexport interface SoftmaxLayerConfig extends LayerConfig {\r\n    axis?: number;\r\n}\r\nexport declare class Softmax extends Layer {\r\n    static className: string;\r\n    readonly axis: number;\r\n    readonly softmax: (t: Tensor, a?: number) => Tensor;\r\n    readonly DEFAULT_AXIS: number;\r\n    constructor(config?: SoftmaxLayerConfig);\r\n    call(inputs: Tensor | Tensor[], kwargs: Kwargs): Tensor | Tensor[];\r\n    computeOutputShape(inputShape: Shape | Shape[]): Shape | Shape[];\r\n    getConfig(): serialization.ConfigDict;\r\n}\r\n"}}
